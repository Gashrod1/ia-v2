{"x_vel":-35.06825827629404,"Cumulative Timesteps":149611104,"Timestep Consumption Time":15.409561919979751,"episode_touches":0,"Overall Steps per Second":2512.529153585355,"z_vel":-5.807739204078397,"Timesteps Collected":50056,"y_vel":-27.140180932578538,"Policy Reward":1873.8608402336256,"total_touches":0,"Collected Steps per Second":11091.530658764419,"_step":6143,"Cumulative Model Updates":17824,"Value Function Update Magnitude":0.07831418514251709,"episode_goals":0,"Policy Entropy":-0.5969775021076202,"Total Iteration Time":19.922554899938405,"Timestep Collection Time":4.5129929799586535,"_runtime":66594,"Value Function Loss":0.5419016281763712,"Policy Update Magnitude":0.07564588636159897,"SB3 Clip Fraction":0.14134666820367178,"_wandb":{"runtime":66594},"Mean KL Divergence":0.011175726462776462,"PPO Batch Consumption Time":2.29529070854187,"_timestamp":1.763036714539938e+09,"total_goals":0}