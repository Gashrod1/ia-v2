{"Overall Steps per Second":8302.78119729545,"total_touches":0,"Mean KL Divergence":0.008158527004222075,"Policy Update Magnitude":0.050633903592824936,"_runtime":98023,"PPO Batch Consumption Time":0.055174668629964195,"Policy Entropy":0.42079509298006695,"_timestamp":1.7630695057076411e+09,"Cumulative Timesteps":332478388,"Collected Steps per Second":10891.48145210034,"Timesteps Collected":50060,"_step":13516,"Cumulative Model Updates":39670,"z_vel":-9.366609297172609,"Timestep Consumption Time":1.433052372187376,"total_goals":0,"episode_touches":0,"SB3 Clip Fraction":0.10477666308482488,"Value Function Loss":0.19962487618128455,"Timestep Collection Time":4.596252605319023,"Value Function Update Magnitude":0.10710445046424866,"y_vel":-18.37574311378023,"_wandb":{"runtime":98023},"Policy Reward":193.45958272175488,"x_vel":28.720470596970543,"episode_goals":0,"Total Iteration Time":6.029304977506399}