{"z_vel":-23.580006098622135,"Value Function Loss":0.05772876491149267,"episode_goals":0,"Collected Steps per Second":12425.16558905781,"y_vel":44.675001865346,"Mean KL Divergence":0.016126648678133886,"_timestamp":1.7630226793540862e+09,"Policy Update Magnitude":0.03223363310098648,"Timestep Consumption Time":15.990223321132362,"Timesteps Collected":50062,"Cumulative Model Updates":14308,"_runtime":53331,"Policy Reward":75.00558798647073,"Total Iteration Time":20.019304434070364,"Overall Steps per Second":2500.6862833256437,"_step":4879,"total_touches":0,"episode_touches":0,"Timestep Collection Time":4.029081112938002,"_wandb":{"runtime":53331},"x_vel":-11.024273452201847,"Cumulative Timesteps":119885422,"PPO Batch Consumption Time":2.3782416184743247,"total_goals":0,"Value Function Update Magnitude":0.09909596294164658,"Policy Entropy":0.5185677210489908,"SB3 Clip Fraction":0.19573667272925377}