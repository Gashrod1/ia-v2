{"Policy Entropy":-0.2733084311087926,"Value Function Update Magnitude":0.10419195145368576,"Total Iteration Time":44.00558529997943,"Collected Steps per Second":1506.5813272717419,"total_touches":0,"episode_goals":0,"episode_touches":0,"Policy Update Magnitude":0.12438743561506271,"_wandb":{"runtime":6020},"_step":1177,"PPO Batch Consumption Time":1.6031681299209595,"SB3 Clip Fraction":0.19685000057021776,"y_vel":-24.043810591942965,"Timestep Collection Time":33.187720499990974,"z_vel":-21.09357178791483,"_timestamp":1.76297336416205e+09,"x_vel":-20.29754679302782,"Policy Reward":259.5424234247927,"Value Function Loss":1.257629652818044,"_runtime":6020,"Timestep Consumption Time":10.817864799988456,"Mean KL Divergence":0.015082957533498606,"total_goals":0,"Timesteps Collected":50000,"Cumulative Model Updates":3514,"Overall Steps per Second":1136.2194062221317,"Cumulative Timesteps":29465438}