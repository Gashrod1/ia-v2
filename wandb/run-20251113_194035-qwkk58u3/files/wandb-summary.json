{"Total Iteration Time":5.759048089385033,"y_vel":35.910004775910714,"_step":11370,"Policy Update Magnitude":0.05287274718284607,"SB3 Clip Fraction":0.09027999639511108,"_timestamp":1.7630628110983343e+09,"Timesteps Collected":50030,"z_vel":-2.690641004855079,"x_vel":-11.210736582382705,"Overall Steps per Second":8687.199555116467,"total_goals":0,"Collected Steps per Second":11696.02203664328,"episode_goals":0,"episode_touches":0,"Mean KL Divergence":0.007023103069514036,"total_touches":0,"Policy Reward":481.06224668472856,"_wandb":{"runtime":90965},"Value Function Update Magnitude":0.10825172066688538,"Cumulative Model Updates":33252,"Timestep Consumption Time":1.481525368988514,"Policy Entropy":0.35233833889166516,"Value Function Loss":0.10041686395804088,"Cumulative Timesteps":278795064,"_runtime":90965,"PPO Batch Consumption Time":0.057134151458740234,"Timestep Collection Time":4.277522720396519}