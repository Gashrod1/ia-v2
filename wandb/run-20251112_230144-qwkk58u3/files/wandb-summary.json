{"Collected Steps per Second":1099.3775643045137,"Value Function Loss":2.0778695344924927,"Timestep Consumption Time":10.466313599958085,"_runtime":17362,"y_vel":-12.777708797182962,"Policy Entropy":0.16392022371292114,"x_vel":-24.215931284797787,"episode_touches":0,"Value Function Update Magnitude":0.11155295372009277,"Mean KL Divergence":0.028863596729934216,"Policy Update Magnitude":0.05998210608959198,"Cumulative Model Updates":5978,"Timesteps Collected":50000,"Overall Steps per Second":893.7094528157883,"Total Iteration Time":55.94659409997985,"episode_goals":0,"_wandb":{"runtime":17362},"total_touches":0,"Policy Reward":427.3279086914413,"_step":2064,"total_goals":0,"z_vel":-23.690722145920713,"Timestep Collection Time":45.48028050002176,"PPO Batch Consumption Time":2.2403061389923096,"Cumulative Timesteps":50226616,"_timestamp":1.7629850832216666e+09,"SB3 Clip Fraction":0.26915499567985535}