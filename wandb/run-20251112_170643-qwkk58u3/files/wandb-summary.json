{"_timestamp":1.7629723038420305e+09,"Total Iteration Time":5.097069610841572,"Timestep Collection Time":3.480223569087684,"_runtime":5102,"x_vel":17.4715578399455,"Policy Update Magnitude":0.09009696543216705,"Timestep Consumption Time":1.6168460417538881,"total_touches":0,"Value Function Loss":1.9044261574745178,"total_goals":0,"Timesteps Collected":50016,"SB3 Clip Fraction":0.16828333089749017,"PPO Batch Consumption Time":0.029356837272644043,"Cumulative Timesteps":28515438,"Cumulative Model Updates":3410,"Policy Entropy":-0.3080231597026189,"_wandb":{"runtime":5102},"Overall Steps per Second":9812.697062958476,"episode_goals":0,"y_vel":-11.442468541475002,"Mean KL Divergence":0.011981284711509943,"Policy Reward":333.71352259199546,"Value Function Update Magnitude":0.1250303089618683,"z_vel":-25.409506369999747,"_step":1139,"episode_touches":0,"Collected Steps per Second":14371.490511200504}